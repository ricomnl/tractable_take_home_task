{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take home task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Output of an Undamaged-Repair-Replace (URR) classifier\n",
    "- per part detects whether the part is undamaged or whether it needs to be repaired or replaced\n",
    "- parts that are lightly damaged are typically repaired and parts that are heavily damaged are typically replaced \n",
    "- undamaged < repair < replace\n",
    "\n",
    "Task:\n",
    "- based on the URR classifier output and some ground-truth metadata, find the right thresholds/ decision boundaries that distinguish the three classes - undamaged, repair and replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/car_parts.jpg\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, auc, precision_recall_fscore_support, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "Claim data:\n",
    "- claim_id: A unique ID for a claim\n",
    "- make: Make description of the vehicle\n",
    "- model: Model description of the vehicle\n",
    "- year: Model year of the vehicle\n",
    "- poi: The main point of impact (eg. Front Centre, Right Rear Corner, etc.)\n",
    "\n",
    "Line data:\n",
    "- line_num: Number of the line item\n",
    "- part: Name of part (eg. fbumper, bbumper, etc.)\n",
    "- operation: Name of operation (eg. repair, replace)\n",
    "- part_price: Total price of the part if replaced in \\\\$\n",
    "- labour_amt: Total labour amount to perform the operation in \\\\$\n",
    "\n",
    "Additional info:\n",
    "- a claim can have multiple line items (the claim data columns will be the same, and the line data columns differ)\n",
    "- the line data contains information on the damaged parts for the claims: the operation (repair or replace) performed on the part and the cost associated with the operations\n",
    "- If there isnâ€™t a line for a part in a claim, assume that that part is undamaged\n",
    "- Also assume that the vehicle details (make, model, year) and point of impact (poi) are known at inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_folder_to_pd(path):\n",
    "    all_files = sorted(glob.glob(path + \"/*.csv\"))\n",
    "\n",
    "    li = []\n",
    "\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        li.append(df)\n",
    "\n",
    "    return pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = read_folder_to_pd(\"./tractable_ds_excercise_data/metadata/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata.sort_values(by=[\"claim_id\", \"line_num\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier output data:\n",
    "- claim_id: A unique ID for the claim\n",
    "- part: Name of part (eg. fbumper, bbumper, etc.)\n",
    "- urr_score: Undamaged / Repair / Replace score, float value ranging from 0 to 1\n",
    "- set: The set (train/val/test) that the claim belongs to, where 0 => train, 1 => val and 2 => test\n",
    "\n",
    "Additional info:\n",
    "- For each claim, the classifiers output scores for all 10 parts, so there will be 10 lines per claim in this file\n",
    "- If the classifier score is missing, that means that the part was not identified by the AI in the images provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_output = pd.read_csv(\"./tractable_ds_excercise_data/classifier_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all NaNs because here NaN means the part was not identified, hence the score is not \"undamaged\" but meaningless\n",
    "clf_output = clf_output.dropna(axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_output.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "- improve the performance of the URR classifier by determining the right thresholds/decision boundaries between the three classes (undamaged/repair/replace)\n",
    "- the purpose of the final system is to predict which parts need to be repaired and replaced in a given claim, but your task stops at finding the thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Determine ground-truth labels from the metadata and merge the two data dumps for analysis (expected content in hand-in: code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = clf_output.merge(metadata[[\"claim_id\", \"part\", \"operation\"]], on=[\"claim_id\", \"part\"], how=\"left\")\n",
    "# here we fill all NaNs with \"undamaged\" because when a line is not in the metadata we can assume the part is undamaged\n",
    "data[\"operation\"] = data[\"operation\"].fillna(\"undamaged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for dataset imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = data[\"operation\"].value_counts()\n",
    "for op, count in class_count.items():\n",
    "    print(f\"{op}: {round(count/sum(class_count.values), 3)*100}%\")\n",
    "class_count.plot(kind=\"pie\", legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = len(data[\"operation\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_TO_LABELS = {}\n",
    "for category, label in enumerate(class_count.keys()):\n",
    "    CAT_TO_LABELS[category] = label\n",
    "    \n",
    "LABELS_TO_CAT = {label: category for category, label in CAT_TO_LABELS.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Analyse the performance of the classifiers (expected content in hand-in: code, 2-5 bullet-points and 1-3 tables/figures)\n",
    "\n",
    "#### Answer:\n",
    "- Because we don't have the threshold yet, we can use either ROC or Precision-Recall curves to analyse how the classifier performed for different threshold\n",
    "- We will use the Precision-Recall curve because as seen in the cell above, we have an imbalanced dataset\n",
    "- We have to use a multiclass Precision-Recall curve as there are 3 classes\n",
    "\n",
    "- The way I am approaching this task is:\n",
    "    - Given a list of URR scores\n",
    "    - For each class (undamaged, repair, replace) I am going through all the URR scores\n",
    "    - For each URR score I am saying: If this is the boundary for this class, what would be the precision, recall, f1 score and precision/recall AUC?\n",
    "    - I can then retrieve the optimal boundary by finding the threshold which produced the best f1 score for each class\n",
    "    - By looking at the precision recall curve and the AUC score I can evaluate its performance without knowing the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train, val and test set\n",
    "train_data = data[data[\"set\"] == 0]\n",
    "val_data = data[data[\"set\"] == 1]\n",
    "test_data = data[data[\"set\"] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiclass_pr(y_pred, y_test, n_classes, figsize=(10, 8), labels={}):\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    f_1_score = {}\n",
    "    thresholds = {}\n",
    "    pr_auc = {}\n",
    "    no_skill = {}\n",
    "\n",
    "    y_test = pd.get_dummies(y_test, drop_first=False).values\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], thresholds[i] = precision_recall_curve(y_test[:, i], y_pred)\n",
    "        f_1_score[i] =  (2 * precision[i] * recall[i]) / (precision[i] + recall[i] + 1e-10)\n",
    "        pr_auc[i] = auc(recall[i], precision[i])\n",
    "        no_skill[i] = np.count_nonzero(y_test[:, 0] == 1) / len(y_test)\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title('Precision Recall curve')\n",
    "    for i in range(n_classes):\n",
    "        best_f1_index = np.argmax(f_1_score[i])\n",
    "        print(f\"Best threshold for label {labels.get(i, i)}: {thresholds[i][best_f1_index]}\")\n",
    "        ax.plot(recall[i], precision[i], \n",
    "                label=f'PR curve (area = {round(pr_auc[i], 2)}) for label {labels.get(i, i)}')\n",
    "        ax.scatter(recall[i][best_f1_index], precision[i][best_f1_index], s=100, marker=(5, 1), \n",
    "                   label=f'Best F1 score ({round(f_1_score[i][best_f1_index], 2)}) for label {labels.get(i, i)}')\n",
    "        ax.plot([0, 1], [no_skill[i], no_skill[i]], linestyle='--')\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.grid(alpha=.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiclass_pr(y_pred=train_data[\"urr_score\"].values, \n",
    "                   y_test=train_data[\"operation\"], \n",
    "                   n_classes=N_CLASSES,\n",
    "                   labels=CAT_TO_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiclass_pr(y_pred=val_data[\"urr_score\"].values, \n",
    "                   y_test=val_data[\"operation\"], \n",
    "                   n_classes=N_CLASSES,\n",
    "                   labels=CAT_TO_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiclass_pr(y_pred=test_data[\"urr_score\"].values, \n",
    "                   y_test=test_data[\"operation\"], \n",
    "                   n_classes=N_CLASSES,\n",
    "                   labels=CAT_TO_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "Find the optimum thresholds to distinguish the undamaged/repair/replace classes. You are free to choose any objective here (accuracy, true positive rate, etc.) but make sure to justify your choice, and that the justification is sensible (expected content in hand-in: code, 2-5 bullet-points and 1-3 tables/figures)\n",
    "\n",
    "#### Answer:\n",
    "- why do I use weighted average\n",
    "- why do I use precision and recall and f1 score\n",
    "- why do I use scipy optimize, could also use linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_true_and_scores(data):\n",
    "    y_true = np.where(data[\"operation\"] == \"replace\", 2, np.where(data[\"operation\"] == \"repair\", 1, 0))\n",
    "    y_scores = data['urr_score'].values\n",
    "    return y_true, y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_pred_w_threshold(y_scores, threshold):\n",
    "    return np.where(y_scores > threshold[1], 2, np.where(y_scores > threshold[0], 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using scipy's optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the training and validation data for tuning\n",
    "y_true_train, y_scores_train = get_y_true_and_scores(train_data.append(val_data))\n",
    "y_true_test, y_scores_test = get_y_true_and_scores(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_to_f1(threshold, y_true, y_scores):\n",
    "    y_pred = get_y_pred_w_threshold(y_scores, threshold)\n",
    "    return -f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "best_thr = scipy.optimize.fmin(threshold_to_f1, args=(y_true_train, y_scores_train), x0=[0.1, 0.9])\n",
    "print(f\"Best threshold: {best_thr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = get_y_pred_w_threshold(y_scores_test, best_thr)\n",
    "\n",
    "precision, recall, f_1_score, _ = precision_recall_fscore_support(y_true_test, y_pred_test, average=\"weighted\")\n",
    "print(f\"Precision: {round(precision, 4)}\\nRecall:    {round(recall, 4)}\\nF1 Score:  {round(f_1_score, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "- Here we are plugging the raw scores of the URR classfier into a Logistic Regression model\n",
    "- We are training the Logistic Regression model using the URR scores as input data and the corresponding class labels as testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the training, validation and testing data\n",
    "y_train_lr, x_train_lr = get_y_true_and_scores(train_data)\n",
    "y_val_lr, x_val_lr = get_y_true_and_scores(val_data)\n",
    "y_test_lr, x_test_lr = get_y_true_and_scores(test_data)\n",
    "\n",
    "x_train_lr = x_train_lr.reshape(-1, 1)\n",
    "x_val_lr = x_val_lr.reshape(-1, 1)\n",
    "x_test_lr = x_test_lr.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the Logistic Regression model (we could use a grid search here to get better parameters)\n",
    "clf_lr = LogisticRegression(C=1.0, penalty='l2')\n",
    "clf_lr.fit(x_train_lr, y_train_lr)\n",
    "clf_lr.score(x_val_lr, y_val_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = clf_lr.predict(x_test_lr)\n",
    "precision, recall, f_1_score, _ = precision_recall_fscore_support(y_test_lr, y_pred_lr, average=\"weighted\")\n",
    "print(f\"Precision: {round(precision, 4)}\\nRecall:    {round(recall, 4)}\\nF1 Score:  {round(f_1_score, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing one example manually\n",
    "example_calculation = (clf_lr.coef_ * x_test_lr[0]).T + clf_lr.intercept_\n",
    "example_calculation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Predicted class: {np.argmax(example_calculation[0])}\\nActual class:    {y_pred_lr[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr.coef_, clf_lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- discuss how to take this solution closer to something that could be used live"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~Task 4~~~\n",
    "- discuss ways you would scale your code to ingest more data, i.e. of the order of millions (expected content in hand-in: 2-5 bullet-points)\n",
    "\n",
    "#### Answer:\n",
    "- use Spark instead of Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~Task 5~~~\n",
    "- discuss ways you would further improve the performance of the classifiers if you had more time (expected content in hand-in: 2-5 bullet-points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "- predict the cost of performing the operations for a particular claim as a function of the URR scores and the claim data\n",
    "- How would you design a system that can predict these costs? \n",
    "- Which metadata fields would you use? \n",
    "- Would you also require any additional data (not provided in the data dump) that will help you improve the accuracy of your estimate? \n",
    "(expected content in hand-in: 5-10 bullet-points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "\n",
    "- Designing a system to predict the costs:\n",
    "    - The model would take the URR scores as well as certain metadata fields as its input data\n",
    "        - $f(URR\\_scores, make, model, year, poi) = cost\\_of\\_operation$\n",
    "    - We would need a regression model\n",
    "        - Popular choices include: linear regression, regression trees, random forest, xgboost, support vector regression, neural network\n",
    "    - \"part_price\" and \"labour_amt\" would be the target columns (it wasn't clear from the description if it's only \"labour_amt\" but I would assume it is because \"part_price\" could be done via a lookup table), we could train a model for each or predict the two with the same model (I would choose to do the latter because there is a lot of overlap in the parameters that are relevant)\n",
    "    - Because both fields can get quite large, we would normalize them (\"learn\" mean and variance from train set and apply to val and test set to avoid data leakage)\n",
    "        - We can then reverse the normalization to get our actual values to generate our report\n",
    "\n",
    "\n",
    "- Metadata fields:\n",
    "    - To choose which metadata fields would be useful we would check which fields are most correlated to the \"part_price\" and \"labour_amount\" (example for \"make\" below)\n",
    "        - If a field (feature) shows close to 0 correlation, it's probably useless\n",
    "    - My intuition tells me that there is probably some correlation between the \"make\" and \"model\" of a car to how much it will cost to repair/replace it. Crashing the trunk of a Tesla is most likely more expensive than crashing the trunk of a Fiat.\n",
    "    - There will also probably be a correlation between \"poi\" and the final price, e.g. fixing the door is more expensive than fixing the back bumper\n",
    "    \n",
    "\n",
    "- Feature engineering:\n",
    "    - If we use a linear regression of neural net:\n",
    "        - Once we have chosen the metadata fields we would need to one-hot-encode our features \"make\", \"model\", \"poi\"\n",
    "            - It wouldn't make sense to label encode them because there is not particular order in any of them\n",
    "            - Because one-hot-encoding will result in a very sparse input vector, we could try out other encoding methods like target encoding or count encoding (target makes more sense here)\n",
    "        - I would also convert year into a numerical representation. I can imagine \"age\" would work well as the parts and labour might get more expensive the older the car is because parts might be more rare. It wouldn't be the amount of years the car has been driven but rather the amount of years since it has been first released.\n",
    "            - The problem we run into here is that if we use (current_year - build_year) as our definition of age, it changes every year and we'd have to retrain our model at least on a yearly basis\n",
    "            - As an alternative we could again, try the target encoding\n",
    "    - If we use a decision tree we don't need to worry as much about encoding the categorical variables as DTs can handle them quite well\n",
    "\n",
    "\n",
    "- Additional data\n",
    "    - It would help to know the country and perhaps even region of the accident, e.g. companies in London will charge more than in Fordwich (which, as of 2011 has 381 inhabitants)\n",
    "    \n",
    " \n",
    "- Process:\n",
    "    - Find a simple baseline (to assure the model is learning something), we could just take something like average cost per part or per make\n",
    "    - Build a simple ML model to beat the baseline (to confirm that ML is even useful for this task)\n",
    "    - Overfit our model (add (bigger) layers, train longer, use bigger model)\n",
    "    - Apply regularization methods and tune hyperparameters (dropout, different architectures, more data, regularization methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labour amount and Part Price for Volkswagen Tiguan (replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = metadata[(metadata[\"make\"] == \"Volkswagen\") & \n",
    "             (metadata[\"model\"] == \"Tiguan\") & \n",
    "             (metadata[\"operation\"] == \"replace\")] \\\n",
    "            [[\"make\", \"model\", \"year\", \"poi\", \"operation\", \"part_price\", \"labour_amt\"]] \\\n",
    "            .dropna() \\\n",
    "            .sort_values(by=[\"year\", \"poi\"])\n",
    "\n",
    "print(t[\"labour_amt\"].describe())\n",
    "t.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labour amount for Volkswagen Tiguan (repair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = metadata[(metadata[\"make\"] == \"Volkswagen\") & \n",
    "             (metadata[\"model\"] == \"Tiguan\") & \n",
    "             (metadata[\"operation\"] == \"repair\")] \\\n",
    "            [[\"make\", \"model\", \"year\",\t\"poi\", \"operation\", \"labour_amt\"]] \\\n",
    "            .dropna() \\\n",
    "            .sort_values(by=[\"year\", \"poi\"])\n",
    "\n",
    "print(t[\"labour_amt\"].describe())\n",
    "t.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between car make - part_price, labour_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"model\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = pd.get_dummies(metadata[[\"make\", \"part_price\", \"labour_amt\"]], columns=[\"make\"]).dropna().corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tractable",
   "language": "python",
   "name": "tractable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
